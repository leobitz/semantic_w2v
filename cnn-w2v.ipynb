{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed_val = 1000\n",
    "random.seed(seed_val)\n",
    "import numpy as np\n",
    "np.random.seed(seed_val)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(seed_val)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, TimeDistributed\n",
    "from keras.layers import Concatenate, Flatten, Lambda\n",
    "from keras.layers import GRU, Conv2D, MaxPooling2D\n",
    "from keras.layers import Input, Reshape, Dot, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from data_handle import *\n",
    "from gensim_wrapper import *\n",
    "import gensim\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def neg_sample_loss(x):\n",
    "    posZ, negZ = x\n",
    "    posZ = K.log(K.sigmoid(posZ))\n",
    "    negZ = K.log(K.sigmoid(-negZ))\n",
    "    \n",
    "    loss = -posZ - negZ\n",
    "    return loss\n",
    "\n",
    "def conv_model_multi(n_chars, n_consonant, n_vowels, n_units):\n",
    "    target_word = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"target_word\")\n",
    "    context_word = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"context_word\")\n",
    "    negative_word = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"negative_word\")\n",
    "    \n",
    "    input_word = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"input_word\")\n",
    "    t = Conv2D(10, (5, 5), padding='same', activation='relu')(input_word)\n",
    "    t = MaxPooling2D(3, 3)(t)\n",
    "    t = Flatten()(t)\n",
    "    t = Dense(n_units, activation='linear')(t)\n",
    "    target_model = Model(input_word, t, name=\"target_model\")\n",
    "    \n",
    "    out_word = Input(shape=(n_chars, (n_consonant + n_vowels), 1), name=\"out_word\")\n",
    "    c = Conv2D(10, (5, 5), padding='same', activation='relu')(out_word)\n",
    "    c = MaxPooling2D(3, 3)(c)\n",
    "    c = Flatten()(c)\n",
    "    c = Dense(n_units, activation='linear')(c)\n",
    "    context_model = Model(out_word, c, name=\"context_model\")\n",
    "    \n",
    "    target = target_model(target_word)\n",
    "    context = context_model(context_word)\n",
    "    negative = context_model(negative_word)\n",
    "    posZ = Dot(axes=1, name=\"tar_dot_con\")([target, context])\n",
    "    negZ = Dot(axes=1, name=\"tar_dot_neg\")([target, negative])\n",
    "    \n",
    "    loss_layer = Lambda(neg_sample_loss, name=\"neg_loss\")([posZ, negZ])\n",
    "    \n",
    "    model = Model([target_word, context_word, negative_word], loss_layer)\n",
    "    return model, target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateSG(data, skip_window, batch_size,\n",
    "\t\t\t   int2word, char2tup, n_chars, n_consonant, n_vowels):\n",
    "\twin_size = skip_window  # np.random.randint(1, skip_window + 1)\n",
    "\ti = win_size\n",
    "\twhile True:\n",
    "\t\tbatch_input = []\n",
    "\t\tbatch_output = []\n",
    "\t\tbatch_vec_input = []\n",
    "\t\tfor bi in range(0, batch_size, skip_window ):\n",
    "\t\t\tcontext = data[i - win_size: i + win_size + 1]\n",
    "\t\t\ttarget = context.pop(win_size)\n",
    "\t\t\tcontext = np.random.choice(context, skip_window, replace=False)\n",
    "\t\t\ttargets = [target] * (win_size )\n",
    "\t\t\tbatch_input.extend(targets)\n",
    "\t\t\tbatch_output.extend(context)\n",
    "\n",
    "\t\t\tcon_mat, vow_mat = word2vec_seperated(char2tup,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t  int2word[target], n_chars, n_consonant, n_vowels)\n",
    "\t\t\tword_mat = np.concatenate([con_mat, vow_mat], axis=1).reshape(\n",
    "\t\t\t\t(1, 1, n_chars, (n_consonant + n_vowels)))\n",
    "\t\t\tbatch_vec_input.extend([word_mat] * (win_size))\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i + win_size + 1 > len(data):\n",
    "\t\t\t\ti = win_size\n",
    "\t\tbatch_vec_input = np.vstack(batch_vec_input)\n",
    "\t\tyield batch_input, batch_vec_input, batch_output\n",
    "\n",
    "\n",
    "words = read_file(\"data/news.txt\")[:2000]\n",
    "words, word2freq = min_count_threshold(words)\n",
    "# words = subsampling(words, 1e-3)\n",
    "vocab, word2int, int2word = build_vocab(words)\n",
    "char2int, int2char, char2tup, tup2char, n_consonant, n_vowel = build_charset()\n",
    "print(\"Words to train: \", len(words))\n",
    "print(\"Vocabs to train: \", len(vocab))\n",
    "#print(\"Unk count: \", word2freq['<unk>'])\n",
    "int_words = words_to_ints(word2int, words)\n",
    "int_words = np.array(int_words, dtype=np.int32)\n",
    "n_chars = 11 + 2\n",
    "n_epoch = 10\n",
    "batch_size = 5\n",
    "skip_window = 3\n",
    "change = 5\n",
    "init_lr = .05\n",
    "gen = generateSG(list(int_words), skip_window, batch_size,\n",
    "\t\t\t\t int2word, char2tup, n_chars, n_consonant, n_vowel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del train_model\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    pass\n",
    "train_model, target_model = conv_model_multi(n_chars, n_consonant, n_vowel, embed_size)\n",
    "adam = keras.optimizers.Nadam(.0005)\n",
    "train_model.compile(optimizer=adam, loss=my_loss)\n",
    "gen = gen_imag_neg(list(int_words), skip_window, batch_size,\n",
    "               int2word, char2tup,ns_unigrams, n_chars, n_consonant, n_vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"em/weight-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "history = train_model.fit_generator(gen, steps_per_epoch=n_batches, epochs = 10, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
